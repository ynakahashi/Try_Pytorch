{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/nn116003/torchtext-tutorial/blob/master/%E8%87%AA%E5%88%86%E3%81%AE%E3%83%87%E3%83%BC%E3%82%BF%E3%81%A7%E3%82%B5%E3%82%AF%E3%83%83%E3%81%A8attention%E3%81%A4%E3%81%8D%E6%96%87%E6%9B%B8%E5%88%86%E9%A1%9E%E3%83%A2%E3%83%87%E3%83%AB%E3%82%92%E3%81%9F%E3%82%81%E3%81%99.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import janome\n",
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "import torchtext\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from torchtext.vocab import FastText\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "j_t = Tokenizer()\n",
    "def tokenizer(text): \n",
    "    return [tok for tok in j_t.tokenize(text, wakati=True)]\n",
    "\n",
    "TEXT = data.Field(sequential=True, tokenize=tokenizer, lower=True, include_lengths=True, batch_first=True)\n",
    "LABEL = data.Field(sequential=False, use_vocab=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, val, test = data.TabularDataset.splits(\n",
    "#         path='./Data/', train='train.tsv',\n",
    "#         validation='valid.tsv', test='test.tsv', format='tsv',\n",
    "#         fields=[('Text', TEXT), ('Label', LABEL)])\n",
    "train, val, test = data.TabularDataset.splits(\n",
    "    path='./Data/', \n",
    "    train='train_small.tsv',\n",
    "    validation='valid_small.tsv', \n",
    "    test='test_small.tsv',\n",
    "    format='tsv',\n",
    "    fields = [('Text', TEXT), ('Label', LABEL)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train) 1045\n",
      "vars(train[0]) {'Text': ['text'], 'Label': 'Label'}\n",
      "len(val) 350\n",
      "vars(val[0]) {'Text': ['text'], 'Label': 'Label'}\n",
      "len(test) 350\n",
      "vars(test[0]) {'Text': ['text'], 'Label': 'Label'}\n"
     ]
    }
   ],
   "source": [
    "print('len(train)', len(train))\n",
    "print('vars(train[0])', vars(train[0]))\n",
    "\n",
    "print('len(val)', len(val))\n",
    "print('vars(val[0])', vars(val[0]))\n",
    "\n",
    "print('len(test)', len(test))\n",
    "print('vars(test[0])', vars(test[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train, min_freq=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FastTextの学習済みデータを利用する場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEXT.build_vocab(train, vectors = FastText(language=\"ja\"), min_freq = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, val_iter, test_iter = data.Iterator.splits(\n",
    "        (train, val, test), batch_sizes=(32, 256, 256), device=\"cpu\", repeat=False, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_iter))\n",
    "print(batch.Text)\n",
    "print(batch.Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch.Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class EncoderRNN(nn.Module):\n",
    "#     def __init__(self, emb_dim, h_dim, v_size, gpu=True, batch_first=True):\n",
    "#         super(EncoderRNN, self).__init__()\n",
    "#         self.gpu = gpu\n",
    "#         self.h_dim = h_dim\n",
    "#         self.embed = nn.Embedding(v_size, emb_dim)\n",
    "#         self.embed.weight.data.copy_(TEXT.vocab.vectors)\n",
    "#         self.lstm = nn.LSTM(emb_dim, h_dim, batch_first=batch_first,\n",
    "#                             bidirectional=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, emb_dim, h_dim, v_size, gpu=True, v_vec=None, batch_first=True):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.gpu = gpu\n",
    "        self.h_dim = h_dim\n",
    "        self.embed = nn.Embedding(v_size, emb_dim)\n",
    "        if v_vec is not None:\n",
    "            self.embed.weight.data.copy_(v_vec)\n",
    "        self.lstm = nn.LSTM(emb_dim, h_dim, batch_first=batch_first,\n",
    "                            bidirectional=True)\n",
    "\n",
    "    def init_hidden(self, b_size):\n",
    "        h0 = Variable(torch.zeros(1*2, b_size, self.h_dim))\n",
    "        c0 = Variable(torch.zeros(1*2, b_size, self.h_dim))\n",
    "        if self.gpu:\n",
    "            h0 = h0.cuda()\n",
    "            c0 = c0.cuda()\n",
    "        return (h0, c0)\n",
    "\n",
    "    def forward(self, sentence, lengths=None):\n",
    "        self.hidden = self.init_hidden(sentence.size(0))\n",
    "        emb = self.embed(sentence)\n",
    "        packed_emb = emb\n",
    "\n",
    "        if lengths is not None:\n",
    "            lengths = lengths.view(-1).tolist()\n",
    "            packed_emb = nn.utils.rnn.pack_padded_sequence(emb, lengths)\n",
    "\n",
    "        out, hidden = self.lstm(packed_emb, self.hidden)\n",
    "\n",
    "        if lengths is not None:\n",
    "            out = nn.utils.rnn.pad_packed_sequence(output)[0]\n",
    "\n",
    "        out = out[:, :, :self.h_dim] + out[:, :, self.h_dim:]\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self, h_dim):\n",
    "        super(Attn, self).__init__()\n",
    "        self.h_dim = h_dim\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(h_dim, 24),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(24,1)\n",
    "        )\n",
    "\n",
    "    def forward(self, encoder_outputs):\n",
    "        b_size = encoder_outputs.size(0)\n",
    "        attn_ene = self.main(encoder_outputs.view(-1, self.h_dim)) # (b, s, h) -> (b * s, 1)\n",
    "        return F.softmax(attn_ene.view(b_size, -1), dim=1).unsqueeze(2) # (b*s, 1) -> (b, s, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnClassifier(nn.Module):\n",
    "    def __init__(self, h_dim, c_num):\n",
    "        super(AttnClassifier, self).__init__()\n",
    "        self.attn = Attn(h_dim)\n",
    "        self.main = nn.Linear(h_dim, c_num)\n",
    "\n",
    "\n",
    "    def forward(self, encoder_outputs):\n",
    "        attns = self.attn(encoder_outputs) #(b, s, 1)\n",
    "        feats = (encoder_outputs * attns).sum(dim=1) # (b, s, h) -> (b, h)\n",
    "        return F.log_softmax(self.main(feats),dim=1), attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epoch, train_iter, optimizer, log_interval=1, batch_size=2):\n",
    "    encoder.train()\n",
    "    classifier.train()\n",
    "    correct = 0\n",
    "    for idx, batch in enumerate(train_iter):\n",
    "        (x, x_l), y = batch.Text, batch.Label\n",
    "        optimizer.zero_grad()\n",
    "        encoder_outputs = encoder(x)\n",
    "        output, attn = classifier(encoder_outputs)\n",
    "        loss = F.nll_loss(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(y.data.view_as(pred)).cpu().sum()\n",
    "        if idx % log_interval == 0:\n",
    "            print('train epoch: {} [{}/{}], acc:{}, loss:{}'.format(\n",
    "            epoch, (idx+1)*len(x), len(train_iter)*batch_size,\n",
    "            correct/float(log_interval * len(x)),\n",
    "            loss.data[0]))\n",
    "            correct = 0\n",
    "\n",
    "            \n",
    "def test_model(epoch, test_iter):\n",
    "    encoder.eval()\n",
    "    classifier.eval()\n",
    "    correct = 0\n",
    "    for idx, batch in enumerate(test_iter):\n",
    "        (x, x_l), y = batch.Text, batch.Label\n",
    "        encoder_outputs = encoder(x)\n",
    "        output, attn = classifier(encoder_outputs)\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(y.data.view_as(pred)).cpu().sum()\n",
    "    print('test epoch:{}, acc:{}'.format(epoch, correct/float(len(test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 300 #単語埋め込み次元\n",
    "h_dim = 3 #lstmの隠れ層の次元\n",
    "class_num = 2 #予測クラウ数\n",
    "lr = 0.001 #学習係数\n",
    "epochs = 50 #エポック数\n",
    "\n",
    " # make model\n",
    "encoder = EncoderRNN(emb_dim, h_dim, len(TEXT.vocab),gpu=False, v_vec = TEXT.vocab.vectors)\n",
    "classifier = AttnClassifier(h_dim, class_num)\n",
    "\n",
    "# init model\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if hasattr(m, 'weight') and (classname.find('Embedding') == -1):\n",
    "        nn.init.xavier_uniform(m.weight.data, gain=nn.init.calculate_gain('relu'))\n",
    "\n",
    "for m in encoder.modules():\n",
    "    print(m.__class__.__name__)\n",
    "    weights_init(m)\n",
    "    \n",
    "for m in classifier.modules():\n",
    "    print(m.__class__.__name__)\n",
    "    weights_init(m)\n",
    "\n",
    "# optim\n",
    "from itertools import chain\n",
    "optimizer = optim.Adam(chain(encoder.parameters(),classifier.parameters()), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "for epoch in range(epochs):\n",
    "    train_model(epoch + 1, train_iter, optimizer)\n",
    "    test_model(epoch + 1, val_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.split(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(TEXT.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.vocab.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
